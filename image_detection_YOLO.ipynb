{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsHKcE4HVHpj3dllLiEtWO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rumeysakeskin/Traffic-Signs-Detection-with-YOLO/blob/main/ts_image_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "READ INPUT IMAGE"
      ],
      "metadata": {
        "id": "knsMrDgO9vtr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcwET4E99tnh"
      },
      "outputs": [],
      "source": [
        "path = \"\"\n",
        "# OpenCV by default reads images in BGR format\n",
        "image_BGR = cv2.imread(path)\n",
        "print('Image shape:', image_BGR.shape)  # tuple of (466, 700, 3)\n",
        "h, w = image_BGR.shape[:2]  \n",
        "\n",
        "# The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob from input image after mean subtraction, normalizing, and RB channels \n",
        "# swapping resulted shape has number of images, number of channels, width and height\n",
        "blob = cv2.dnn.blobFromImage(image_BGR, 1 / 255.0, (416, 416), swapRB=True, crop=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD YOLOV3 NETWORK"
      ],
      "metadata": {
        "id": "HDIrKjGF9078"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('classes.names') as f:\n",
        "    labels = [line.strip() for line in f]\n",
        "\n",
        "# Loading trained YOLO v3 Objects Detector\n",
        "# with the help of 'dnn' library from OpenCV\n",
        "network = cv2.dnn.readNetFromDarknet('darknet/cfg/yolov3_train.cfg',\n",
        "                                      'darknet/weights/yolov3_train_1000.weights')\n",
        "\n",
        "# Getting list with names of all layers from YOLO v3 network\n",
        "layers_names_all = network.getLayerNames()\n",
        "\n",
        "print(layers_names_all)\n",
        "\n",
        "# Getting only output layers' names that we need from YOLO v3 algorithm with function that returns indexes of layers with unconnected outputs\n",
        "layers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
        "\n",
        "print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n",
        "\n",
        "# Setting minimum probability to eliminate weak predictions\n",
        "probability_minimum = 0.5\n",
        "\n",
        "# Setting threshold for filtering weak bounding boxes with non-maximum suppression\n",
        "threshold = 0.3\n",
        "\n",
        "# Generating colours for representing every detected object with function randint(low, high=None, size=None, dtype='l')\n",
        "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
        "\n",
        "# print(type(colours))  # <class 'numpy.ndarray'>\n",
        "# print(colours.shape)  # (80, 3)\n",
        "# print(colours[0])  # [172  10 127]"
      ],
      "metadata": {
        "id": "IFTAeNdL90eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network.setInput(blob)  # setting blob as input to the network\n",
        "start = time.time()\n",
        "output_from_network = network.forward(layers_names_output)\n",
        "end = time.time()\n",
        "\n",
        "# Showing spent time for forward pass\n",
        "print()\n",
        "print('Objects Detection took {:.5f} seconds'.format(end - start))"
      ],
      "metadata": {
        "id": "rq4Hgc6V96r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET BOUNDING BOXES"
      ],
      "metadata": {
        "id": "qvnDxkPT97LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing lists for detected bounding boxes, obtained confidences and class's number\n",
        "bounding_boxes = []\n",
        "confidences = []\n",
        "class_numbers = []\n",
        "\n",
        "# Going through all output layers after feed forward pass\n",
        "for result in output_from_network:\n",
        "    # Going through all detections from current output layer\n",
        "    for detected_objects in result:\n",
        "        # Getting 80 classes' probabilities for current detected object\n",
        "        scores = detected_objects[5:]\n",
        "        # Getting index of the class with the maximum value of probability\n",
        "        class_current = np.argmax(scores)\n",
        "        # Getting value of probability for defined class\n",
        "        confidence_current = scores[class_current]\n",
        "\n",
        "        # # Every 'detected_objects' numpy array has first 4 numbers with\n",
        "        # # bounding box coordinates and rest 80 with probabilities for every class\n",
        "        # print(detected_objects.shape)  # (85,)\n",
        "\n",
        "        # Eliminating weak predictions with minimum probability\n",
        "        if confidence_current > probability_minimum:\n",
        "            # Scaling bounding box coordinates to the initial image size\n",
        "            # YOLO data format keeps coordinates for center of bounding box\n",
        "            # and its current width and height\n",
        "            # That is why we can just multiply them elementwise\n",
        "            # to the width and height\n",
        "            # of the original image and in this way get coordinates for center\n",
        "            # of bounding box, its width and height for original image\n",
        "            box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
        "\n",
        "            # Now, from YOLO data format, we can get top left corner coordinates\n",
        "            # that are x_min and y_min\n",
        "            x_center, y_center, box_width, box_height = box_current\n",
        "            x_min = int(x_center - (box_width / 2))\n",
        "            y_min = int(y_center - (box_height / 2))\n",
        "\n",
        "            # Adding results into prepared lists\n",
        "            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
        "            confidences.append(float(confidence_current))\n",
        "            class_numbers.append(class_current)"
      ],
      "metadata": {
        "id": "cPcXgqW898lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NON-MAXIMUM SUPPRESSION"
      ],
      "metadata": {
        "id": "TkQsFPRE-Ax9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing non-maximum suppression of given bounding boxes\n",
        "# With this technique we exclude some of bounding boxes if their\n",
        "# corresponding confidences are low or there is another\n",
        "# bounding box for this region with higher confidence\n",
        "\n",
        "# It is needed to make sure that data type of the boxes is 'int'\n",
        "# and data type of the confidences is 'float'\n",
        "# https://github.com/opencv/opencv/issues/12789\n",
        "results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,\n",
        "                            probability_minimum, threshold)"
      ],
      "metadata": {
        "id": "xvNIuBs1-Fsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DRAWING BOUNDING BOXES AND LABELS"
      ],
      "metadata": {
        "id": "magKBBoW-Ine"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining counter for detected objects\n",
        "counter = 1\n",
        "\n",
        "# Checking if there is at least one detected object after non-maximum suppression\n",
        "if len(results) > 0:\n",
        "    # Going through indexes of results\n",
        "    for i in results.flatten():\n",
        "        # Showing labels of the detected objects\n",
        "        print('Object {0}: {1}'.format(counter, labels[int(class_numbers[i])]))\n",
        "\n",
        "        # Incrementing counter\n",
        "        counter += 1\n",
        "\n",
        "        # Getting current bounding box coordinates,\n",
        "        # its width and height\n",
        "        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
        "        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
        "\n",
        "        # Preparing colour for current bounding box\n",
        "        # and converting from numpy array to list\n",
        "        colour_box_current = colours[class_numbers[i]].tolist()\n",
        "\n",
        "        # # # Check point\n",
        "        # print(type(colour_box_current))  # <class 'list'>\n",
        "        # print(colour_box_current)  # [172 , 10, 127]\n",
        "\n",
        "        # Drawing bounding box on the original image\n",
        "        cv2.rectangle(image_BGR, (x_min, y_min),\n",
        "                      (x_min + box_width, y_min + box_height),\n",
        "                      colour_box_current, 2)\n",
        "\n",
        "        # Preparing text with label and confidence for current bounding box\n",
        "        text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\n",
        "                                                confidences[i])\n",
        "\n",
        "        # Putting text with label and confidence on the original image\n",
        "        cv2.putText(image_BGR, text_box_current, (x_min, y_min - 5),\n",
        "                    cv2.FONT_HERSHEY_COMPLEX, 0.7, colour_box_current, 2)\n",
        "\n",
        "# Comparing how many objects where before non-maximum suppression and left after\n",
        "print()\n",
        "print('Total objects been detected:', len(bounding_boxes))\n",
        "print('Number of objects left after non-maximum suppression:', counter - 1)\n",
        "\n",
        "\n",
        "# Saving resulted image in jpg format by OpenCV \n",
        "cv2.imwrite('result.jpg', image_BGR)"
      ],
      "metadata": {
        "id": "BsNlW7xJ-JQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
