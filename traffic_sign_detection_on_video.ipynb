{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/6/zVy6rT59HrfghFqMNy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rumeysakeskin/YOLO-Darknet-Video-Image-Detection-Traffic-Signs/blob/main/traffic_sign_detection_on_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s_AXnq7EI4Rx"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD YOLOV3 NETWORK**"
      ],
      "metadata": {
        "id": "x0MO5NN1QhAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('classes.names') as f:\n",
        "    labels = [line.strip() for line in f]\n",
        "\n",
        "# Loading trained YOLO v3 Objects Detector\n",
        "# with the help of 'dnn' library from OpenCV\n",
        "network = cv2.dnn.readNetFromDarknet('yolov3_test.cfg',\n",
        "                                      'yolov3_train_3000.weights')\n",
        "\n",
        "# Getting list with names of all layers from YOLO v3 network\n",
        "layers_names_all = network.getLayerNames()\n",
        "\n",
        "# Getting only output layers' names that we need from YOLO v3 algorithm with function that returns indexes of layers with unconnected outputs\n",
        "layers_names_output = [layers_names_all[i - 1] for i in network.getUnconnectedOutLayers()]\n",
        "\n",
        "print(layers_names_output)  # Detections at layers: ['yolo_82', 'yolo_94', 'yolo_106']\n",
        "\n",
        "# Setting minimum probability to eliminate weak predictions\n",
        "probability_minimum = 0.5\n",
        "\n",
        "# Setting threshold for filtering weak bounding boxes with non-maximum suppression\n",
        "threshold = 0.3\n",
        "\n",
        "# Generating colours for representing every detected object with function randint(low, high=None, size=None, dtype='l')\n",
        "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-lqdnXPNr3F",
        "outputId": "c4256bab-34c5-4278-90eb-da5c2b7ef1ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yolo_82', 'yolo_94', 'yolo_106']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBJECT DETECTION ON VIDEO**"
      ],
      "metadata": {
        "id": "TnLnaBKHWy_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capture = cv2.VideoCapture(\"/content/traffic-sign-test.mp4\") \n",
        "ret = True\n",
        "# Preparing variable for writer that we will use to write processed frames\n",
        "writer= None\n",
        "# Preparing variables for spatial dimensions of the frames\n",
        "h, w = None, None\n",
        "# Defining variable for counting frames at the end we will show total amount of processed frames\n",
        "f = 0\n",
        "# Defining variable for counting total time at the end we will show time spent for processing all frames\n",
        "t = 0\n",
        "\n",
        "while ret: # Break the loop if the video has ended\n",
        "    ret, frame = capture.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    if w is None or h is None:  \n",
        "      h, w = frame.shape[:2]\n",
        "\n",
        "    # blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)     \n",
        "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "    network.setInput(blob)  # setting blob as input to the network\n",
        "    start = time.time()\n",
        "    output_from_network = network.forward(layers_names_output)\n",
        "    end = time.time()\n",
        "\n",
        "    f += 1\n",
        "    t += end - start\n",
        "\n",
        "    # Preparing lists for detected bounding boxes, obtained confidences and class's number\n",
        "    bounding_boxes = []\n",
        "    confidences = []\n",
        "    class_numbers = []\n",
        "\n",
        "    # Going through all output layers after feed forward pass\n",
        "    for result in output_from_network:\n",
        "        # Going through all detections from current output layer\n",
        "        for detected_objects in result:\n",
        "            # Getting 4 classes' probabilities for current detected object\n",
        "            scores = detected_objects[5:]\n",
        "            # Getting index of the class with the maximum value of probability\n",
        "            class_current = np.argmax(scores)\n",
        "            # Getting value of probability for defined class\n",
        "            confidence_current = scores[class_current]\n",
        "\n",
        "            # Eliminating weak predictions with minimum probability\n",
        "            if confidence_current > probability_minimum:\n",
        "                # Scaling bounding box coordinates to the initial image size\n",
        "                # YOLO data format keeps coordinates for center of bounding box\n",
        "                # and its current width and height\n",
        "                # That is why we can just multiply them elementwise\n",
        "                # to the width and height\n",
        "                # of the original image and in this way get coordinates for center\n",
        "                # of bounding box, its width and height for original image\n",
        "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
        "\n",
        "                # Now, from YOLO data format, we can get top left corner coordinates\n",
        "                # that are x_min and y_min\n",
        "                x_center, y_center, box_width, box_height = box_current\n",
        "                x_min = int(x_center - (box_width / 2))\n",
        "                y_min = int(y_center - (box_height / 2))\n",
        "\n",
        "                # Adding results into prepared lists\n",
        "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
        "                confidences.append(float(confidence_current))\n",
        "                class_numbers.append(class_current)\n",
        "\n",
        "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
        "\n",
        "    counter = 1\n",
        "\n",
        "    # Checking if there is at least one detected object after non-maximum suppression\n",
        "    if len(results) > 0:\n",
        "        # Going through indexes of results\n",
        "        for i in results.flatten():\n",
        "            # Showing labels of the detected objects\n",
        "\n",
        "            # Incrementing counter\n",
        "            counter += 1\n",
        "\n",
        "            # Getting current bounding box coordinates,\n",
        "            # its width and height\n",
        "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
        "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
        "\n",
        "            # Preparing colour for current bounding box\n",
        "            # and converting from numpy array to list\n",
        "            colour_box_current = colours[class_numbers[i]].tolist()\n",
        "\n",
        "            # Drawing bounding box on the original image\n",
        "            cv2.rectangle(frame, (x_min, y_min),\n",
        "                          (x_min + box_width, y_min + box_height),\n",
        "                          colour_box_current, 2)\n",
        "\n",
        "            # Preparing text with label and confidence for current bounding box\n",
        "            text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\n",
        "                                                    confidences[i])\n",
        "\n",
        "            # Putting text with label and confidence on the original image\n",
        "            cv2.putText(frame, text_box_current, (x_min, y_min - 3),\n",
        "                        cv2.FONT_HERSHEY_COMPLEX, 0.5, colour_box_current, 1)\n",
        "            \n",
        "\n",
        "    # Writing processed frame into the file\n",
        "    # Initializing writer\n",
        "    # we do it only once from the very beginning when we get spatial dimensions of the frames\n",
        "    if writer is None:\n",
        "        # Constructing code of the codec to be used in the function VideoWriter\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "        # Writing current processed frame into the video file\n",
        "        writer = cv2.VideoWriter('result_traffic_sign.mp4', fourcc, 30,\n",
        "                                 (frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "    # Write processed current frame to the file\n",
        "    writer.write(frame)\n",
        "\n",
        "\n",
        "print()\n",
        "print('Total number of frames', f)\n",
        "print('Total amount of time {:.5f} seconds'.format(t))\n",
        "\n",
        "# Releasing video reader and writer\n",
        "capture.release()\n",
        "writer.release()\n",
        "\n",
        "\"\"\"\n",
        "Parameters for cv2.VideoWriter():\n",
        "    filename - Name of the output video file.\n",
        "    fourcc - 4-character code of codec used to compress the frames, colour or pixel format used in media files.\n",
        "    fps\t- Frame rate of the created video.\n",
        "    frameSize - Size of the video frames.\n",
        "    isColor\t- If it True, the encoder will expect and encode colour frames.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "qPUmtlG1I-55",
        "outputId": "627da551-8768-4182-8e46-119f02c8624b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of frames 56\n",
            "Total amount of time 70.96405 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nParameters for cv2.VideoWriter():\\n    filename - Name of the output video file.\\n    fourcc - 4-character code of codec used to compress the frames, colour or pixel format used in media files.\\n    fps\\t- Frame rate of the created video.\\n    frameSize - Size of the video frames.\\n    isColor\\t- If it True, the encoder will expect and encode colour frames.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}